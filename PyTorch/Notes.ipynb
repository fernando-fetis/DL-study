{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8903e170",
   "metadata": {},
   "source": [
    "# <center>PyTorch</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec518f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e458ecd",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779f553b",
   "metadata": {},
   "source": [
    "### Módulos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bc140e",
   "metadata": {},
   "source": [
    "#### `nn.Sequential`\n",
    "\n",
    "* Es un contenedor ordenado de módulos.\n",
    "* Tiene el método de append.\n",
    "* También se le puede pasar un `collections.OrderedDict` con los nombres.\n",
    "* Su forward es la concatenación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cc5563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "net = nn.Sequential(OrderedDict([\n",
    "    ('hidden_linear', nn.Linear(10, 50)),\n",
    "    ('hidden_activation', nn.Tanh()),\n",
    "    ('output_linear', nn.Linear(50, 2))\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8fcff7",
   "metadata": {},
   "source": [
    "#### `nn.ModuleList`\n",
    "\n",
    "* Solo es una lista como la de python para almacenar módulos. La diferencia está en que de esta forma se registran los módulos y sus parámetros en la red. con la lista de python no.\n",
    "* Con `nn.ModuleDict` se hace lo mismo solo que se registran los nombres."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d07146",
   "metadata": {},
   "source": [
    "#### Manejo de módulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311e7518",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Linear()\n",
    "net.get_parameter('block1.0.weight')  # obtener un parametro por su nombre.\n",
    "net.get_submodule('nombre')  # obtener un bloque por su nombre.\n",
    "net.modules()  # iterador sobre los módulos. Con named_modules se tiene una tupla con el nombre del módulo.\n",
    "\n",
    "# ver modulos directos de la red:\n",
    "for i in net.children(): # con named_children() se tiene una tupla con el nombre del módulo.\n",
    "    print(i)\n",
    "    \n",
    "# Agregar módulos en loop a una red:\n",
    "class model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        modules = [...]\n",
    "        for module in modules:\n",
    "            self.add_module('nombre', module) # register_module(name, module) es alias de add_module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177e14c0",
   "metadata": {},
   "source": [
    "### Parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa58604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros de un modelo lineal:\n",
    "net = nn.Linear(10, 15)\n",
    "params = list(net.parameters()) # 2 elementos (pesos y biases).\n",
    "net.weight  # parámetros de peso.\n",
    "net.bias  # parámetros de bias.\n",
    "net.bias.grad  # gradiente del bias.\n",
    "\n",
    "# Recorrer parámetros:\n",
    "for param in net.parameters():\n",
    "    pass\n",
    "for name, param in net.named_parameters():\n",
    "    pass\n",
    "    \n",
    "# Número de parametros:\n",
    "n_params = sum(p.numel() for p in net.parameters())\n",
    "\n",
    "# Resumen del modelo:\n",
    "import torchsummary as ts\n",
    "ts.summary(net, (10,))\n",
    "\n",
    "# Agregar parámetro al módulo:\n",
    "net.register_parameter('nombre', param)\n",
    "\n",
    "# Obtener los parametros de una red (implementación):\n",
    "class model():\n",
    "    def __init__(self):\n",
    "        ...\n",
    "    def get_parameters(self):\n",
    "        for key, value in self.__dict__.items():\n",
    "            if type(value) == nn.Parameter:\n",
    "                print(key, 'es un parámetro.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c0cb6d",
   "metadata": {},
   "source": [
    "### Implementación de dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63089931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(layer, p, train=True):\n",
    "    if train:\n",
    "        if p == 1:\n",
    "            return torch.zeros_like(layer)\n",
    "        mask = (torch.rand(layer.shape) > p).float()\n",
    "        layer = layer * mask/(1.0-p)\n",
    "    return layer\n",
    "\n",
    "# Ejemplo:\n",
    "h = torch.randn(10**5).to(dtype=torch.float)\n",
    "h_dropout = dropout(h, p=0.1, train=True)\n",
    "print('La media de cada neurona es la misma en ambas capas:', h-h_dropout)\n",
    "\n",
    "# En el forward debe ser aplicada siempre y se debe indicar si está en train o eval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b95537",
   "metadata": {},
   "source": [
    "### Otros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43feb228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Forward funcional:\n",
    "x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "\n",
    "# View en vez de flatten:\n",
    "x = x.view(-1, 5*5*64)\n",
    "\n",
    "# Resumen del modelo:\n",
    "import torchsummary as ts\n",
    "ts.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b063d6e4",
   "metadata": {},
   "source": [
    "## Tensores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4313d91",
   "metadata": {},
   "source": [
    "### Creación de tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d13e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.Tensor(5)  # tensor lleno con ceros. 1d de tamaño 5.\n",
    "tensor = torch.Tensor(5, 3)  # tensor lleno con ceros. 2d de tamaño 5x3.\n",
    "\n",
    "tensor = torch.zeros(2, 3)  # o bien [2, 3].\n",
    "tensor = torch.ones(2, 3)\n",
    "\n",
    "tensor = torch.arange(10) # constructor usando rango. tensor [0, ..., 9].\n",
    "tensor = torch.arange(3, 11, 2) # start, end, step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ade003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling de U[0,1):\n",
    "tensor = torch.rand(2, 3, 4)\n",
    "tensor = torch.rand([2, 3, 4])\n",
    "\n",
    "# Sampling de N(0,1):\n",
    "tensor = torch.randn(2, 3, 4)\n",
    "tensor = torch.randn([2, 3, 4])\n",
    "\n",
    "# Sampling de enteros en [a,b):\n",
    "tensor = torch.randint(3, 10, [5])  # no permite unpacking de size ya que tiene más argumentos.\n",
    "tensor = torch.randint(10, [5])  # low=0.\n",
    "\n",
    "# Sampling de una normal con distintos parámetros:\n",
    "tensor1, tensor2 = torch.normal(mean=torch.tensor([0., 5]), std=torch.tensor([1, 1e-2]))\n",
    "\n",
    "# Copiar tamaño de otro vector:\n",
    "tensor2 = torch.rand_like(tensor1)  # llena sampleando de U[0,1).\n",
    "tensor2 = torch.randn_like(tensor1)  # llena sampleando de N(0,1).\n",
    "tensor2 = torch.randint_like(tensor1, 10)  # llena con enteros menores a 10.\n",
    "\n",
    "# Permutación aleatoria de índices:\n",
    "tensor = torch.randperm(10) # mezcla los enteros de 0 al 9.\n",
    "\n",
    "# Operaciones in-place:\n",
    "tensor.normal_(0, 1)  # rellena el tensor con sampleos en N(0,1).\n",
    "tensor.random_(1, 100)  # rellena con sampleos en U[1,100). Si se deja abierto el lado derecho, se limita por su datatype.\n",
    "tensor.uniform_(1, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e0bca1",
   "metadata": {},
   "source": [
    "### `device` y `dtype`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e296fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensores a GPU:\n",
    "tensor = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]], device='cuda')\n",
    "tensor = tensor.to(device='cuda')  # o cuda:0.\n",
    "tensor = tensor.cuda()  # o cuda(0). Es alias para to(device='cuda').\n",
    "torch.cuda.get_device_name(0)  # entrega el nombre del dispositivo por defecto.\n",
    "\n",
    "# Tensor float dtype:\n",
    "tensor(1.)\n",
    "tensor(1, dtype=torch.float)\n",
    "tensor(1, dtype=torch.float32)\n",
    "\n",
    "# Tensor 1D a float:\n",
    "float_num = float(tensor)\n",
    "\n",
    "# Distintas formas de fijar el data type:\n",
    "tensor = torch.tensor([[1, 2], [3, 4]], dtype=torch.short)\n",
    "tensor = torch.tensor([[1, 2], [3, 4]]).to(dtype=torch.short)\n",
    "tensor = torch.tensor([[1, 2], [3, 4]]).short()  # alias para to(dtype=torch.short).\n",
    "tensor = int(torch.tensor([[1, 2], [3, 4]]))  # equivale a hacerlo usando to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e82c81",
   "metadata": {},
   "source": [
    "### Operatoria sobre tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362126db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor.resize_(100, 10)  # se puede cambiar las dimensiones de un tensor.\n",
    "tensor.zero_()  # se puede asignar puros ceros a un tensor.\n",
    "tensor.fill_(1)  # o llenar con cualquier valor.\n",
    "tensor.mul_(2)  # multiplica por 2 element-wise.\n",
    "tensor = tensor.sqrt()  # se puede aplicar una función a cada elemento del tensor.\n",
    "tensor = tensor[None] # añadir dimensión al comienzo (con img_tensor[None, None] se agregan dos dimensiones).\n",
    "\n",
    "# Tensores como referencia a otros tensores:\n",
    "tensor2 = torch.Tensor(tensor)  # al cambiar tensor2 también cambia \"tensor\". Eso es porque al crear tensor2 en realidad se crea una referencia del tensor \"tensor\".\n",
    "tensor3 = tensor.clone()  # si se quiere copiar un tensor, pero no como referencia, se usa el método clone.\n",
    "\n",
    "tensor.numel()  # numel devuelve la cantidad de elementos totales en un tensor.\n",
    "tensor.dim()  # dim devuelve el número de dimensiones del tensor.\n",
    "tensor.view(1, -1) # flatten a un tensor.\n",
    "\n",
    "# Transpuesta:\n",
    "tensor.t()\n",
    "tensor.transpose(0,1)\n",
    "\n",
    "# Productos:\n",
    "tensor1 * tensor2  # element-wise multiplication. Del mismo modo se tendría w**2 element-wise.\n",
    "tensor1 @ tensor2  # producto punto entre dos vectores 1d."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c60644f",
   "metadata": {},
   "source": [
    "### Otros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b1b6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se puede acceder a los elementos de distintas formas:\n",
    "tensor[0][2][1]  # notación C/C++.\n",
    "tensor[0, 2, 1]  # notación Matlab.\n",
    "\n",
    "tensor[:,1] # acceder a los elementos de una columna.\n",
    "tensor[:,[1]]  # columna como un tensor 2D.\n",
    "\n",
    "# torch.stack([A,B], dim = 0) es equivalente a torch.cat([A.unsqueeze(0), b.unsqueeze(0)], dim = 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190d2222",
   "metadata": {},
   "source": [
    "### Named tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6b1ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.tensor([1,2,3], names=['dim0'])\n",
    "tensor = torch.tensor([[1,2,3], [4,5,6]], names=['dim0', 'dim1'])\n",
    "\n",
    "# Agregar nombres a un tensor existente:\n",
    "tensor = torch.tensor([[1,2,3], [4,5,6]])\n",
    "tensor_named = tensor.refine_names(..., 'dim1') # deja en None el nombre de la primera dimensión.\n",
    "\n",
    "# Alinear dimensiones como otro tensor:\n",
    "tensor1 = torch.randn((3,28,28), names=['C', 'H', 'W'])\n",
    "tensor2 = torch.randn((28,28,3), names=['H', 'W', 'C'])\n",
    "tensor_aligned = tensor1.align_as(tensor2)\n",
    "\n",
    "# Funciones que aceptan argumentos de dimensión permiten named dimensions:\n",
    "suma_canales = tensor.sum('C')\n",
    "\n",
    "# Si se intentan combinar dimensiones de distinto nombre se obtiene un error:\n",
    "tensor_suma = tensor1 + tensor2\n",
    "\n",
    "# Quitarle el nombre a un tensor:\n",
    "tensor.rename(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72811865",
   "metadata": {},
   "source": [
    "### Broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fff7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si uno de los operadores tiene dimensiones adicionales, el otro operador se repetirá en cada dimensión adicional:\n",
    "a = torch.rand(2, 3, 4)\n",
    "b = torch.rand(   3, 4)\n",
    "\n",
    "(a*b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2f1d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Es necesario que la compatibilidad de dimensiones sea desde la derecha:\n",
    "a = torch.rand(2, 3, 4)\n",
    "b = torch.rand(2, 3)\n",
    "\n",
    "try: (a*b).shape\n",
    "except: print('No permitido.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f764d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si uno de los operadores tiene alguna dimensión de tamaño 1, se repetirá el operador en cada componente de la dimensión respectiva del otro operador:\n",
    "a = torch.rand(5, 2, 3, 4, 1)\n",
    "b = torch.rand(   2, 1, 4, 5)\n",
    "\n",
    "(a*b + 1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a359ea",
   "metadata": {},
   "source": [
    "### Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c86af9c",
   "metadata": {},
   "source": [
    "#### Cómputo de gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa998d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.tensor(10., requires_grad=True)\n",
    "x2 = torch.tensor(100., requires_grad=True)\n",
    "\n",
    "print(x1.grad, x2.grad)  # inicialmente los gradientes de x1 y x2 son None.\n",
    "\n",
    "y = 3*x1 + 4*x2 + 5\n",
    "\n",
    "# Por construcción, el tensor 'y' también irá guardando el grafo computacional para regresar hasta x1 y x2 en cadenas más largas:\n",
    "print(y.requires_grad)  # si x1, x2 no guardaran grafo, y tampoco lo haría (basta que al menos uno lo haga).\n",
    "\n",
    "# Se guarda la derivada (en cada parámetro) de quien hace el backward:\n",
    "y.backward()  # se actualizarán los gradientes de x1 y x2 a dy/dx.\n",
    "\n",
    "print(x1.grad, x2.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d649102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se acumulan (suman) los gradientes si no se reinician:\n",
    "\n",
    "x = torch.tensor(10., requires_grad=True)\n",
    "\n",
    "y1 = 3*x\n",
    "y1.backward()  # se debería hacer x.grad.zero_()\n",
    "\n",
    "y2 = 5*x\n",
    "y2.backward()\n",
    "\n",
    "print(x.grad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ef9e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como tensor 1D:\n",
    "x = torch.tensor([10., 100], requires_grad=True)\n",
    "y = torch.tensor([3., 4]) @ x\n",
    "\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c7db17",
   "metadata": {},
   "source": [
    "#### Regla de la cadena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1238ddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.tensor(10., requires_grad=True)\n",
    "x2 = torch.tensor(100., requires_grad=True)\n",
    "x3 = torch.tensor(1000., requires_grad=True)\n",
    "\n",
    "y = 3*x1 + 4*x2 + 5\n",
    "z = 6*y + 7*x3\n",
    "# En los nodos hoja se guardará dz/dx. Para x1 y x2 se calculará como dz/dx = dz/dy * dy/dx.\n",
    "z.backward()\n",
    "# Gradientes propagados:\n",
    "print(x1.grad, x2.grad, x3.grad, y.grad)  # no se actualiza el gradiente para nodos intermedios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83be0614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En un backward solo se actualizan los gradientes para los nodos hoja (para ahorrar memoria). Es una decisión de diseño.\n",
    "# Para actualizar los gradientes para los nodos intermedios, se debe indicar con retain_grad.\n",
    "\n",
    "x1 = torch.tensor(10., requires_grad=True)\n",
    "x2 = torch.tensor(100., requires_grad=True)\n",
    "x3 = torch.tensor(1000., requires_grad=True)\n",
    "\n",
    "y = 3*x1 + 4*x2 + 5\n",
    "y.retain_grad()\n",
    "z = 6*y + 7*x3\n",
    "z.backward()\n",
    "print(x1.grad, x2.grad, x3.grad, y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f2d1e0",
   "metadata": {},
   "source": [
    "#### Operaciones in-place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6967331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para operaciones in-place, se debe usar no_grad ya que pytorch no puede rastrear el cambio (puede necesitar valores que ya no están disponibles):\n",
    "\n",
    "x = torch.tensor(5., requires_grad=True)\n",
    "\n",
    "with torch.no_grad():  # sin esto se tendría un error.\n",
    "    x += 2\n",
    "\n",
    "y = x**2\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "\n",
    "# Si se hiciera una operación in-place despúes crear un nuevo tensor, no se podría propagar gradiente desde ese tensor:\n",
    "\n",
    "x = torch.tensor(5., requires_grad=True)\n",
    "y = x**2\n",
    "\n",
    "with torch.no_grad():  # sin esto se tendría un error.\n",
    "    x += 2\n",
    "\n",
    "try: y.backward()\n",
    "except: print('No permitido.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec44a5b2",
   "metadata": {},
   "source": [
    "#### Funcionamiento del optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bed7092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "param = torch.tensor(100.0, requires_grad=True)\n",
    "\n",
    "optimizer = SGD([param], lr=0.5)\n",
    "\n",
    "output = 10 * param\n",
    "optimizer.zero_grad() # reinicia los gradientes de los tensores registrados en el optimizador.\n",
    "output.backward()\n",
    "\n",
    "# Actualización:\n",
    "optimizer.step()\n",
    "print(param)\n",
    "\n",
    "# Otra actualización (sin haber actualizado gradientes):\n",
    "optimizer.step()\n",
    "print(param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
