{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPIP7OfQ8LJf"
      },
      "source": [
        "# Clasificador binario de Reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfltVy128LJk"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmlVOr888LJl"
      },
      "source": [
        "## Data cleansing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvzT3lz-BbRQ"
      },
      "outputs": [],
      "source": [
        "# reviews y labels son un solo string que almacena todo el archivo (cada instancia está separada por \\n):\n",
        "\n",
        "with open('data/reviews.txt', 'r') as f:\n",
        "  reviews = f.read()\n",
        "with open('data/labels.txt', 'r') as f:\n",
        "  labels = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87bm9QOaCDmX",
        "outputId": "ffaf7017-216c-405f-f377-0652fef0a305"
      },
      "outputs": [],
      "source": [
        "from string import punctuation\n",
        "from collections import Counter\n",
        "\n",
        "# Se eliminarán los signos de puntuación:\n",
        "reviews = reviews.lower()\n",
        "reviews = ''.join(c for c in reviews if c not in punctuation)\n",
        "\n",
        "# Se separarán las reviews por instancia:\n",
        "sequences_str = reviews.split('\\n')\n",
        "labels_str = labels.split('\\n')\n",
        "print('Número de secuencias:', len(sequences_str))\n",
        "\n",
        "# Se obtiene el vocabulario:\n",
        "vocab_freq = Counter(' '.join(sequences_str).split())  # dict de la forma {palabra: frecuencia}.\n",
        "vocab = sorted(vocab_freq.keys(), key=vocab_freq.get, reverse=True)  # lista con las palabras (str) ordenadas por frecuencia.\n",
        "print('Tamaño vocabulario:', len(vocab))\n",
        "\n",
        "# Se asocia cada palabra con un índice:\n",
        "vocab2idx = {word: i for i, word in enumerate(vocab, 1)}  # dict de la forma {palabra: int_asociado}.\n",
        "\n",
        "# Str a int en secuencias y etiquetas:\n",
        "labels_int = [1 if label == 'positive' else 0 for label in labels_str]  # Se codifican las categorías con 0 o 1:\n",
        "sequences_int = []  # lista con las secuencias. Cada secuencia es una lista (de tamaño variable) de enteros representando palabras.\n",
        "for seq in sequences_str:\n",
        "  sequences_int.append([vocab2idx[word] for word in seq.split()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuZ-S1JwIsQe",
        "outputId": "8a336dcd-28e1-4d6e-f798-8078c4323b3a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Eliminar las reviews de largo 0:\n",
        "non_zero_idx = [i for (i, seq) in enumerate(sequences_int) if len(seq) != 0]\n",
        "sequences_int = [sequences_int[i] for i in non_zero_idx]\n",
        "labels_int = [labels_int[i] for i in non_zero_idx]\n",
        "\n",
        "# Secuencias de tamaño uniforme:\n",
        "def padding_and_clipping(sequences_list, seq_length):\n",
        "    sequences = torch.zeros((len(sequences_list), seq_length), dtype=int)\n",
        "    for i, seq in enumerate(sequences_list):\n",
        "        sequences[i, -len(seq):] = torch.Tensor(seq)[:seq_length]\n",
        "    return sequences\n",
        "\n",
        "seq_length = int(np.median([len(x) for x in sequences_int]))\n",
        "sequences_int = padding_and_clipping(sequences_int, seq_length)\n",
        "\n",
        "print('Tamaño mediano de las secuencias:', seq_length)\n",
        "print('Tamaño final de la data:', sequences_int.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thrKRmiY8LJp"
      },
      "source": [
        "## Datasets and dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juH-uLSa8LJp",
        "outputId": "3e60e5c9-5215-4393-f920-c5e60511b213"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
        "\n",
        "TRAIN_RATIO, DEV_RATIO = 0.8, 0.1\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "dataset = TensorDataset(sequences_int, torch.Tensor(labels_int).to(int))\n",
        "\n",
        "# Partición de la data:\n",
        "def split_dataset(dataset, train_ratio, dev_ratio):\n",
        "    n = len(dataset)\n",
        "    n_train, n_dev = int(n*train_ratio), int(n*dev_ratio)\n",
        "    n_test = n - n_train - n_dev\n",
        "    return random_split(dataset, [n_train, n_dev, n_test])\n",
        "\n",
        "dataset_split = split_dataset(dataset, train_ratio=TRAIN_RATIO, dev_ratio=DEV_RATIO)\n",
        "print('\\nTrain/dev/test datasets size:', [len(dataset) for dataset in dataset_split])\n",
        "\n",
        "dataloaders = {mode: DataLoader(dataset_split[i], batch_size=BATCH_SIZE, shuffle=True)\n",
        "               for i, mode in enumerate(('train', 'dev', 'test'))}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8oTHpbxOcoA"
      },
      "source": [
        "## Red neuronal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3amSRw7QSHD"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, num_classes=2, dropout=0.5):\n",
        "        \n",
        "        super(SentimentRNN, self).__init__()\n",
        "        \n",
        "        # Parámetros para instanciar los estados iniciales de la LSTM:\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings = vocab_size + 1,  # tamaño del diccionario original (se suma el token de padding).\n",
        "            embedding_dim=embedding_dim  # dimensión de embedding para cada entrada.\n",
        "            )\n",
        "        \n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,  # dimensión de entrada a la red.\n",
        "            hidden_size=hidden_size,  # dimensión de los estados c<t> y h<t>.\n",
        "            num_layers=num_layers,  # capas LSTM superpuestas.\n",
        "            batch_first=True,\n",
        "            dropout=dropout,  # dropout para las capas internas.\n",
        "            )\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x, initial_states):  # se esperará que la red reciba el input y los estados iniciales de la LSTM.\n",
        "        \n",
        "        # x.shape = (batch_size, seq_lenght) es una lista de secuencias donde cada secuencia es una lista de enteros. Cada entero representa una palabra.\n",
        "        x = self.embedding(x)\n",
        "        # x.shape = (batch_size, seq_lenght, input_size = embedding_dim). Contiene la lista de secuencias.\n",
        "        \n",
        "        # initial_states = (h<0>, c<0>), cada uno de tamaño (n_layers, batch_size, hidden_size).\n",
        "        output, final_states = self.lstm(x, initial_states)\n",
        "        # output.shape = (batch_size, seq_lenght, hidden_size). Para cada instancia del batch, los outputs generados son los estados ocultos (en cada tiempo) de la última capa.\n",
        "        # final_states = (h<n>, c<n>). Cada uno es de tamaño (num_layers, batch_size, hidden_size) y contiene los últimos states generados (en el último instante de tiempo).\n",
        "                \n",
        "        # Para cada secuencia, solo se necesita la salida que se generó en el último instante de tiempo, no las intermedias:\n",
        "        output = output[:, -1, :]  # shape: (batch_size, hidden_size).\n",
        "        \n",
        "        # Clasificación:\n",
        "        output = self.dropout(output)\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "    \n",
        "    def init_hidden(self, batch_size, device):\n",
        "        h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
        "        c_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
        "        return h_0, c_0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IV3WtMJTYi5",
        "outputId": "a3312645-ae48-497d-dfc8-557d0f392c58"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(vocab)\n",
        "\n",
        "EMBEDDING_DIM = 400  # reducción de dimensionalidad del vocabulario.\n",
        "HIDDEN_SIZE = 256  # dimensión estados de la LSTM.\n",
        "NUM_LAYERS = 2  # número de capas LSTM.\n",
        "LR = 0.001\n",
        "\n",
        "net = SentimentRNN(vocab_size, EMBEDDING_DIM, HIDDEN_SIZE, NUM_LAYERS)\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
        "\n",
        "# Ejemplo:\n",
        "x, y = next(iter(dataloaders['train']))\n",
        "initial_states = net.init_hidden(BATCH_SIZE, 'cpu')\n",
        "print('Tamaño batch:', x.shape)\n",
        "print('Tamaño estados iniciales:', initial_states[0].shape)\n",
        "\n",
        "output = net(x, initial_states)\n",
        "print('\\nTamaño salida:', output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QG74D0RDfLNA"
      },
      "source": [
        "# Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ootGfDVmdR0d"
      },
      "outputs": [],
      "source": [
        "def train_model(net, dataloaders, optimizer, epochs=5, clip=5):\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    net.to(device)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{epochs}')\n",
        "    \n",
        "        for mode in ('train', 'dev'):\n",
        "            \n",
        "            net.train(mode == 'train')\n",
        "            epoch_loss, epoch_accuracy = 0.0, 0.0\n",
        "\n",
        "            with torch.set_grad_enabled(mode == 'train'):\n",
        "              for x, y in dataloaders[mode]:\n",
        "                  x, y = x.to(device), y.to(device)\n",
        "                  initial_states = net.init_hidden(x.shape[0], device)\n",
        "\n",
        "                  output = net(x, initial_states)\n",
        "                  loss = nn.CrossEntropyLoss()(output, y)\n",
        "                  epoch_loss += loss * x.shape[0]\n",
        "                  \n",
        "                  if mode == 'train':\n",
        "                      net.zero_grad()\n",
        "                      loss.backward()\n",
        "                      nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "                      optimizer.step()\n",
        "\n",
        "                  preds = output.argmax(dim=1).to(device)\n",
        "                  epoch_accuracy += (preds == y).sum()\n",
        "            \n",
        "            epoch_loss /= len(dataloaders[mode].dataset)\n",
        "            epoch_accuracy *= 100 / len(dataloaders[mode].dataset)\n",
        "            \n",
        "            print(f'{mode:5} | loss: {epoch_loss:.2f} - accuracy: {epoch_accuracy:.2f}%')\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCJsxkIa8LJs",
        "outputId": "7d1eb9f3-c6e2-4ccf-c9c5-42601dd6d766"
      },
      "outputs": [],
      "source": [
        "train_model(net, dataloaders, optimizer, epochs=5)\n",
        "\n",
        "states = {'net': net.state_dict(),\n",
        "          'optimizer': optimizer.state_dict()}\n",
        "torch.save(states, 'model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2uMK73bgIm7"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLuTiHGD_nFL",
        "outputId": "d57718e3-18f6-46ea-e36b-d2ee717e941e"
      },
      "outputs": [],
      "source": [
        "def test_model(net, dataloader):\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    net.to(device)\n",
        "      \n",
        "    net.eval()\n",
        "    total_loss, total_accuracy = 0.0, 0.0\n",
        "\n",
        "    with torch.set_grad_enabled(False):\n",
        "        for x, y in dataloader:\n",
        "            \n",
        "            x, y = x.to(device), y.to(device)\n",
        "            initial_states = net.init_hidden(x.shape[0], device)\n",
        "\n",
        "            output = net(x, initial_states).squeeze()\n",
        "            loss = nn.CrossEntropyLoss()(output, y)\n",
        "            preds = output.argmax(dim=1).to(device)\n",
        "            \n",
        "            total_loss += loss * x.shape[0]\n",
        "            total_accuracy += (preds == y).sum()\n",
        "    \n",
        "    total_loss /= len(dataloader.dataset)\n",
        "    total_accuracy *= 100 / len(dataloader.dataset)\n",
        "    \n",
        "    print(f'- Loss: {total_loss:.2f}')\n",
        "    print(f'- Accuracy: {total_accuracy:.2f}%')\n",
        "\n",
        "print('Desempeño en el test set:')\n",
        "test_model(net, dataloaders['test'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8Fd8Or6g2Fx"
      },
      "source": [
        "## Inferencia sobre nueva data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6IRHh3deeWA"
      },
      "outputs": [],
      "source": [
        "def tokenize_review(review_str):\n",
        "    review_str = review_str.lower() \n",
        "    review_str = ''.join([c for c in review_str if c not in punctuation])\n",
        "    review_list_str = review_str.split()\n",
        "    review_list_int = [vocab2idx[word] for word in review_list_str]\n",
        "    \n",
        "    return review_list_int\n",
        "\n",
        "# Para decodificar secuencia: text = ' '.join([vocab[i-1] for i in seq)\n",
        "\n",
        "def predict(net, review, seq_length=seq_length):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    net.to(device)\n",
        "    net.eval()\n",
        "    review_batch = padding_and_clipping(sequences_list=[tokenize_review(review)],\n",
        "                                        seq_length=seq_length)\n",
        "    initial_states = net.init_hidden(batch_size=1, device=device)\n",
        "    output = net(review_batch.to(device), initial_states)\n",
        "    fidelity, pred = torch.softmax(output, dim=1).max(dim=1)\n",
        "    fidelity = round(fidelity.item() * 100, 2)\n",
        "    pred = ('negativo', 'positivo')[pred]\n",
        "    \n",
        "    print(f'Sentimiento: {pred} (confianza: {fidelity}%)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRV3pv6PepcS",
        "outputId": "2bc1e619-fcd9-4be9-ae3d-e42413f0e2f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentimiento: positivo (confianza: 67.3%)\n",
            "Sentimiento: negativo (confianza: 85.82%)\n",
            "Sentimiento: positivo (confianza: 77.23%)\n",
            "Sentimiento: positivo (confianza: 91.84%)\n"
          ]
        }
      ],
      "source": [
        "predict(net, 'My cats are called Yuki and Simba. They are both very pretty although Yuki is a bit bored. However, I love them very much.')\n",
        "predict(net, 'This is the most boring year of my career.')\n",
        "predict(net, 'Natural language processing is easier than reinforcement learning.')\n",
        "predict(net, 'This net is as good at detecting sarcasm as a communist is at economics.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "v42xWF71CGSp",
        "yD3COp6YECo4",
        "hcx0QqRgIUrq",
        "NzFm6PGJI1So",
        "LxEk6_qLKnZD",
        "96r_MDRgN6eS",
        "caV4lvLXOMsY",
        "C8oTHpbxOcoA"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2 (v3.10.2:a58ebcc701, Jan 13 2022, 14:50:16) [Clang 13.0.0 (clang-1300.0.29.30)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
